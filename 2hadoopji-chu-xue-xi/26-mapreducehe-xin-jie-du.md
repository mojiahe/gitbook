# 2.6 MapReduce核心解读

---

# 1 **经典问题**

* 提出问题

不借助工具，如何对一个超大文件进行单词统计以及对结果进行排序输出？

* 解决思路

在单机环境下，要想对一个超大文件进行统计，首先想到的是先由主线程去对文件进行等份分割，然后启用多线程去分别统计，各线程把最后得到的统计结果在内部先进行快速排序得到一个有序的结果，主线程再在汇总多个统计结果的时候，使用归并排序将结果合并。

* 涉及的排序方法

快速排序、归并排序、堆排序

* 问题总结

在单机环境下，对超大文件进行统计，这是一个最基本和最有效的解决思路。利用快速排序和归并排序对统计结果进行排序是这个解决思路的核心。

# 2 **MapReduce核心流程**

![](/assets/mapreduce流程.jpg)

## 2.1 **前期阶段**

首先，大文件被存储到HDFS中，并且HDFS会以block文件块的形式将文件分割存储，在hadoop1.0中这个block的大小默认是64MB，在hadoop2.0中默认是128MB。

## 2.2 **Map阶段**

### 2.2.1 **过程总揽**

input--&gt;map--&gt;partition--&gt;sort--&gt;spill--&gt;merge--&gt;combine

### 2.2.2 **Input过程**

Hadoop会在存储有输入数据的节点上启动map任务（数据本地化优化），这时会将读入的文件（block）再次分割成input split，然后按行读取作为map任务的输入。

这里要注意的是split大小的设计问题，如果split的设计得太大，那么会导致split跨越两个block，而同一个节点同时拥有这两个block的几率较低（在集群节点数量较多时），这就需要使用网络传输来将数据传输到该map task节点；如果split设计得太小，又会增加了多次的磁盘IO，降低整体处理速度。因此split最佳大小=block大小。

### 2.2.3 **Map过程**

Map任务将输入数据处理成key/value形式输入到parition。

### 2.2.4 **Partition过程**

对输入的key/value进行默认partition函数处理后将结果（partition/key/value）输出到内存缓冲区（默认大小100MB）。

### 2.2.5 **Spill过程**

当内存缓冲区不断堆积到达默认的阀值（80%，也即是80MB）时，该缓冲区内的这80%的空间将会被锁定。另外20%的空间仍然在接收写入的数据，当这20%的空间被写满的时候而那80%的空间还没有完成写入到磁盘，那么map的输出结果将会写入到一个队列，等待80%被清空后再进行写入。而80%被锁定时，会启动Spill线程对这个内存空间进行持久化到磁盘的操作。

但是Spill之前需要对80%空间内的数据进行排序。

### 2.2.6 **Sort过程**

排序时先按照Partition进行排序，再按照key进行排序，默认排序算法是快速排序。

![](/assets/map.png)

### 2.2.7 **Merge过程**

当spill溢出操作（spill文件最少会有一个）完最后一个文件时，开始进行merge阶段，也就是多个小文件通过多次归并排序逐渐合并成一个大文件。这时每个spill文件内部都是根据partition和key排好序了的，所以进行归并后的文件也是根据partition和key排好序了。结果类似{“aaa”, \[5, 8, 2,…\]}。

![](/assets/merge.png)

### 2.2.8 **Combiner过程**

Merge完之后，如果用户设置了combiner，那么这时候就会对merge之后的大文件针对相同的key合并结果，也就是{“aaa”, \[5, 8, 2,…\]}合并成{“aaa”, 15}。这是map阶段真正的全部完成。

## 2.3 **Reduce阶段**

### 2.3.1 **过程总揽**

简单地说，reduce task在执行之前的工作就是不断地fetch当前job里每个map task的最终结果，然后对从不同地方fetch过来的数据不断地做merge，也最终形成一个文件作为reduce task的输入文件。见下图：

![](/assets/reduce.png)

### 2.3.2 **Copy过程**

简单地拉取数据。Reduce进程启动一些数据copy线程\(Fetcher\)，通过HTTP方式请求map task所在的TaskTracker获取map task在磁盘的输出文件。当有map task结束，这些文件就开始copy。

### 2.3.3 **Merge阶段**

这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。这里需要强调的是，merge有三种形式：（1）内存到内存；（2）内存到磁盘；（归并排序）（3）磁盘到磁盘（堆排序,大文件合并）。默认情况下第一种形式不启用，让人比较困惑，是吧。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge，与map端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的reduce输入文件。

### 2.3.4 **Reduce过程**

不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。至于怎样才能让这个文件出现在内存中，之后的再看。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。

# 3 **总结**

MapReduce过程中，除了相关的资源监控和任务调度，最重要并且贯彻始终的还是排序算法在其中发挥的作用。总结一下使用了的三种排序算法：

* **一次快速排序**

map阶段的spill操作之前，对被锁定的80%的内存空间根据partition和key进行排序。

* **两次归并排序**

第一次是map阶段的merge操作，将多个spill小文件合并成大的spill文件；

第二次是在reduce阶段的merge操作，将磁盘中多个spill小文件合并成大的spill文件。

* **一次堆排序**

reduce阶段将合并到最后的两个spill大文件合并成最终reduce输入文件。

参考博客：[http://langyu.iteye.com/blog/992916](http://langyu.iteye.com/blog/992916)

[http://www.cnblogs.com/hipercomer/p/4516581.html](http://www.cnblogs.com/hipercomer/p/4516581.html)

